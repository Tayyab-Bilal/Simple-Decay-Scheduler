{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decay scheduler",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5lpdEAflWzF"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import sys\n",
        "import os\n",
        " \n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqpG3l0tmQ2j"
      },
      "source": [
        "# construct the argument parse and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "\n",
        "scheuler_type=\"poly\" #“step”, “linear”, “poly”, \"standard\"\n",
        "num_epochs=0\n",
        "out_lr_plot=\"\"\n",
        "out_train_plot=\"\"\n",
        "\n",
        "# args = vars(ap.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVq-R49z8EzB"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "class LearningRateDecay:\n",
        "\tdef plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
        "\t\t# compute the set of learning rates for each corresponding\n",
        "\t\t# epoch\n",
        "\t\tlrs = [self(i) for i in epochs]\n",
        "\t\t# the learning rate schedule\n",
        "\t\tplt.style.use(\"ggplot\")\n",
        "\t\tplt.figure()\n",
        "\t\tplt.plot(epochs, lrs)\n",
        "\t\tplt.title(title)\n",
        "\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\tplt.ylabel(\"Learning Rate\")\n",
        "\n",
        "\n",
        "class StepDecay(LearningRateDecay):\n",
        "\tdef __init__(self, initAlpha=0.01, factor=0.25, dropEvery=10):\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.factor = factor\n",
        "\t\tself.dropEvery = dropEvery\n",
        "\n",
        "\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the learning rate for the current epoch\n",
        "\t\texp = np.floor((1 + epoch) / self.dropEvery)\n",
        "\t\talpha = self.initAlpha * (self.factor ** exp)\n",
        "\t\t# return the learning rate\n",
        "\t\treturn float(alpha)\n",
        "  \n",
        "class PolynomialDecay(LearningRateDecay):\n",
        "\tdef __init__(self, maxEpochs=100, initAlpha=0.01, power=1.0):\n",
        "\t\tself.maxEpochs = maxEpochs\n",
        "\t\tself.initAlpha = initAlpha\n",
        "\t\tself.power = power\n",
        "\n",
        "\tdef __call__(self, epoch):\n",
        "\t\t# compute the new learning rate based on polynomial decay\n",
        "\t\tdecay = (1 - (epoch / float(self.maxEpochs))) ** self.power\n",
        "\t\talpha = self.initAlpha * decay\n",
        "\t\t# return the new learning rate\n",
        "\t\treturn float(alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG-4U4Uh3gBz",
        "cellView": "form"
      },
      "source": [
        "#@title Resnet\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import add\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "class ResNet:\n",
        "\t@staticmethod\n",
        "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "\t\t# the shortcut branch of the ResNet module should be\n",
        "\t\t# initialize as the input (identity) data\n",
        "\t\tshortcut = data\n",
        "\n",
        "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
        "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(data)\n",
        "\t\tact1 = Activation(\"relu\")(bn1)\n",
        "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
        "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
        "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(conv1)\n",
        "\t\tact2 = Activation(\"relu\")(bn2)\n",
        "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
        "\t\t\tpadding=\"same\", use_bias=False,\n",
        "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
        "\n",
        "\t\t# the third block of the ResNet module is another set of 1x1\n",
        "\t\t# CONVs\n",
        "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(conv2)\n",
        "\t\tact3 = Activation(\"relu\")(bn3)\n",
        "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
        "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
        "\n",
        "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
        "\t\t# the shortcut\n",
        "\t\tif red:\n",
        "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
        "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "\t\t# add together the shortcut and the final CONV\n",
        "\t\tx = add([conv3, shortcut])\n",
        "\n",
        "\t\t# return the addition as the output of the ResNet module\n",
        "\t\treturn x\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, stages, filters,\n",
        "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
        "\t\t# initialize the input shape to be \"channels last\" and the\n",
        "\t\t# channels dimension itself\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# set the input and apply BN\n",
        "\t\tinputs = Input(shape=inputShape)\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(inputs)\n",
        "\n",
        "\t\t# check if we are utilizing the CIFAR dataset\n",
        "\t\tif dataset == \"cifar\":\n",
        "\t\t\t# apply a single CONV layer\n",
        "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "\n",
        "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
        "\t\telif dataset == \"tiny_imagenet\":\n",
        "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
        "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
        "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\t\tmomentum=bnMom)(x)\n",
        "\t\t\tx = Activation(\"relu\")(x)\n",
        "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
        "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "\t\t# loop over the number of stages\n",
        "\t\tfor i in range(0, len(stages)):\n",
        "\t\t\t# initialize the stride, then apply a residual module\n",
        "\t\t\t# used to reduce the spatial size of the input volume\n",
        "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
        "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
        "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "\t\t\t# loop over the number of layers in the stage\n",
        "\t\t\tfor j in range(0, stages[i] - 1):\n",
        "\t\t\t\t# apply a ResNet module\n",
        "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
        "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "\t\t# apply BN => ACT => POOL\n",
        "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(x)\n",
        "\t\tx = Activation(\"relu\")(x)\n",
        "\t\tx = AveragePooling2D((8, 8))(x)\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tx = Flatten()(x)\n",
        "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
        "\t\tx = Activation(\"softmax\")(x)\n",
        "\n",
        "\t\t# create the model\n",
        "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "AFBKt8lRYrfZ",
        "outputId": "e5460014-8dc8-4037-963a-0aa93576f323"
      },
      "source": [
        "#@title Creating callbacks\n",
        "\n",
        "\n",
        "callbacks = []\n",
        "schedule = None\n",
        "\n",
        "if scheuler_type == \"step\":\n",
        "\tprint(\"[INFO] using 'step-based' learning rate decay...\")\n",
        "\tschedule = StepDecay(initAlpha=1e-1, factor=0.25, dropEvery=15)\n",
        " \n",
        "elif scheuler_type == \"linear\":\n",
        "\tprint(\"[INFO] using 'linear' learning rate decay...\")\n",
        "\tschedule = PolynomialDecay(maxEpochs=num_epochs, initAlpha=1e-1, power=1)\n",
        " \n",
        "elif scheuler_type == \"poly\":\n",
        "\tprint(\"[INFO] using 'polynomial' learning rate decay...\")\n",
        "\tschedule = PolynomialDecay(maxEpochs=num_epochs, initAlpha=1e-1, power=5)\n",
        "\n",
        "if schedule is not None:\n",
        "\tcallbacks = [LearningRateScheduler(schedule)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] using 'polynomial' learning rate decay...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay6KVHOrm_L-",
        "outputId": "3d77e37d-f60a-4996-8380-52b646f9722d"
      },
      "source": [
        "\n",
        "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
        "\n",
        "trainX = np.array([cv2.resize(x, (32, 32)) for x in trainX])\n",
        "testX = np.array([cv2.resize(x, (32, 32)) for x in testX])\n",
        "\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "\n",
        "# trainX = trainX.reshape((trainX.shape[0], 32, 32, 1))\n",
        "# testX = testX.reshape((testX.shape[0], 32, 32, 1))\n",
        "\n",
        "\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "lb = LabelBinarizer()\n",
        "trainY = lb.fit_transform(trainY)\n",
        "testY = lb.transform(testY)\n",
        "\n",
        "\n",
        "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "\t\"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "# construct the image generator for data augmentation\n",
        "# aug = ImageDataGenerator(width_shift_range=0.1,\theight_shift_range=0.1, horizontal_flip=True,\tfill_mode=\"nearest\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRx5WmwFrOTF"
      },
      "source": [
        "\n",
        "decay = 0.0\n",
        "\n",
        "if scheuler_type == \"standard\":\n",
        "\tprint(\"[INFO] using 'keras standard' learning rate decay...\")\n",
        "\tdecay = 1e-1 / epochs\n",
        "    \n",
        "elif schedule is None:\n",
        "\tprint(\"[INFO] no learning rate schedule being used\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyOgDqEEb4q2",
        "outputId": "864e2781-e48f-4adf-e2d1-5cb073b39ebe"
      },
      "source": [
        "opt = SGD(lr=1e-1, momentum=0.9, decay=decay)\n",
        "model = ResNet.build(32, 32, 3, 10, (9, 9, 9),\n",
        "\t(64, 64, 128, 256), reg=0.0005)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa7LVqrNcDgE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}